# Erasing Conceptual Knowledge from Language Models
###  [Project Website](https://elm.baulab.info) | [Arxiv Preprint](https://arxiv.org/pdf/2311.12092.pdf) | [Trained Models](https://sliders.baulab.info/weights/xl_sliders/) <br>

<div align='center'>
<img src = 'images/method.png'>
</div>

## Setup
To set up your python environment:
```
conda create -n elm python=3.9
conda activate elm

git  clone https://github.com/rohitgandikota/erasing-llm.git
cd erasing-llm
pip install -r requirements.txt
```

## Erasing WMDP Bio and Cyber threat from a Language Model
```
cd trainscripts
python erase.py --dataset_idx '0,0,1' --model_id 'HuggingFaceH4/zephyr-7b-beta' --num_samples 3000 --eta 1000 --experiment_name 'zephyr-elm-wmdp'
```
The trained elm peft model will be saved to `./elm_models/zephyr-elm-wmdp/checkpoint-final` folder. 

## Erasing Harry Potter concept from a Language Model
```
cd trainscripts
python erase.py --lora_rank 256 --eta 1000 --num_samples 5000 --dataset_idx '2,2,2'  --model_id 'meta-llama/Llama-2-7b-chat-hf' --experiment_name 'llama2-elm-hp'
```
The trained elm peft model will be saved to `./elm_models/llama2-elm-hp/checkpoint-final` folder. 

## ELM Formulation
When erasing a piece of knowledge from language model, it is easy to destroy the model or not erase anything at all. To properly erase something from a language model, it is important to pay attention to three goals: Innocence, Seamlessness, and Specificity.<br>

<b>Innocence</b>: the erased model should not exhibit any traces of knowledge. <b>Seamlessness</b>: the model should not generate gibberish text upon encountering the concept, but rather act like it has never heard of it. <b>Specificity</b>: the erasure should not effect the general capabilities of the original model.<br>

We introduce a new method called <b>Erasure of Language Memory (ELM)</b>. 
To erase a concept from language model, we contruct multiple objectives in the following way:<br>
The unlearnt model should act in a way that it's probability to generate a sequence of text is same as the original model, but with a reduced likelihood that the text contains the concept `c_n` (e.g. "expert in bioweapons") and increased likelihood of concept `c_p` (e.g. "novice in bioweapons") . 
```
P'(x)  = P(x) (P(c_p|x)/ P(c_n|x))^eta
```
where `x` is the text and `c` is the concept we wish to erase. This equation can be simplified using bayes theorem and taking log:
```
log P'(x) α log P(x) + eta * (log P(x|c_p) - log P(x|c_n))
```
Where `P(x|c)` can be characterised as the probability of the text `x` being generated by the model with the context `c` as prefix. For an autoregressive model, this can be expanded as 
```
log P'(xi|x1:xi-1) α log P(xi|x1:xi-1) + eta * (log P(xi|c_p, x1:xi-1) - log P(xi|c_n, x1:xi-1))
```

Similarly we train the model to maintain the general capabilities by using a retain set. 

Finally, we train the model to maintain fluency under attack. i.e. we want the model to be fluent when prompted for the erased concept. Please refer the paper for more details.