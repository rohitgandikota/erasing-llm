# llm-erasing
Repo for building erasing/unlearning method for language models

To run our method 
```
cd trainscripts
python erase.py --rank 32 --eta 1000
```
### Formulation
To erase a concept from language model, we contruct an objective in the following way:<br>
The unlearnt model should act in a way that it's probability to generate a sequence of text is same as the original model, but with a reduced likelihood that the text contains the concept we wish to unlearn.
```
P'(x)  = P(x) / P(c|x)^eta
```
where `x` is the text and `c` is the concept we wish to erase. This equation can be simplified using bayes theorem and taking log:
```
log P'(x) α log P(x) - eta * (log P(x|c) - log P(x))
```
Where `P(x|c)` can be characterised as the probability of the text `x` being generated by the model with the context `c` as prefix. For an autoregressive model, this can be expanded as 
```
log P'(xi|x1:xi-1) α log P(xi|x1:xi-1) - eta * (log P(xi|c_p, x1:xi-1) - log P(xi|c_n, x1:xi-1))
```
